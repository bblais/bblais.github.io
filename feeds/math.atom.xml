<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>bblais on the web - math</title><link href="https://bblais.github.io/" rel="alternate"></link><link href="https://bblais.github.io/feeds/math.atom.xml" rel="self"></link><id>https://bblais.github.io/</id><updated>2021-05-13T00:00:00-04:00</updated><entry><title>Stats 101 Examples with MCMC Part 3</title><link href="https://bblais.github.io/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/" rel="alternate"></link><published>2021-05-13T00:00:00-04:00</published><updated>2021-05-13T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-05-13:/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/</id><summary type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;Estimating mean with known ùúé&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/"&gt;Estimating mean with unknown ùúé&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all of ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;Estimating mean with known ùúé&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/"&gt;Estimating mean with unknown ùúé&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all of these posts I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade&lt;/span&gt;
&lt;span class="err"&gt;pip install emcee&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example the data take the form of a number of "successes", &lt;span class="math"&gt;\(h\)&lt;/span&gt;, in a total number of "trials", &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and we want to estimate the proportion, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.  This could be the number of heads observed in &lt;span class="math"&gt;\(N\)&lt;/span&gt; coin flips and we want to estimate the bias in the coin -- what is the probability that it flips heads.  In fact, the data I will use below is from &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents"&gt;Lindley, 1976. "Inference for a Bernoulli Process (A Bayesian View)"&lt;/a&gt;, where he flips a thumbtack which might be able to produce two states pointing "up" or pointing "down", and gets the data UUUDUDUUUUUD (3 D, 9 U).&lt;/p&gt;
&lt;h2 id="setting-up-the-problem"&gt;Setting up the problem&lt;/h2&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined from the Bernoulli function,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
P(h|N,\theta) &amp;amp;\sim&amp;amp; \text{Bernoulli}(N,\theta) \\
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the location parameters.  This would translate to 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
P(\theta) &amp;amp;\sim&amp;amp; \text{Uniform}(0,1)
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;However there are cases where we have more information.  Even flipping a biased coin, we know that both outcomes (heads and tails) are at least &lt;em&gt;possible&lt;/em&gt; to happen.  Given this, the probabilities &lt;span class="math"&gt;\(P(\theta=0)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(\theta=1)\)&lt;/span&gt; should be zero and a different prior is justified (sidenote -- the maximum entropy solution to this is a Beta(2,2) distribution or, in other words, assume one heads and one tails have been observed before the data).  I'll continue with this example with uniform priors, because in &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents"&gt;Lindley, 1976. "Inference for a Bernoulli Process (A Bayesian View)"&lt;/a&gt; he is flipping a particular thumbtack for which it might not be possible for it to, say, flip "down".  His data turns out to be 3 down, 9 up, so we know after the data it was possible for both outcomes but not &lt;em&gt;before&lt;/em&gt; the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Œ∏&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;logbernoullipdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Œ∏&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MCMCModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;Œ∏&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="running-mcmc-looking-at-the-results"&gt;Running MCMC, looking at the results&lt;/h2&gt;
&lt;p&gt;Now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_mcmc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_chains&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;Sampling Prior...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;0.27 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 1/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.63 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 2/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.79 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 3/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.47 s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/proportion chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_distributions&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/proportion distribution.png"&gt;&lt;/p&gt;
&lt;p&gt;Is this evidence for a biased coin at, say, 95% level?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Œ∏ &amp;lt;0.5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;0.95485&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;yes, just as &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents"&gt;Lindley, 1976 presents.&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="comparing-to-textbook-examples"&gt;Comparing to textbook examples&lt;/h2&gt;
&lt;p&gt;The textbook only considers large sample proportions, with the definition &lt;span class="math"&gt;\(f= h/N\)&lt;/span&gt;. From the Beta distribution we get our estimate of the proportion &lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\hat{\theta} &amp;amp;\sim&amp;amp; f \\
\sigma^2 &amp;amp;\sim&amp;amp; \frac{f\cdot (1-f)}{N}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
which matches the equations in the standard textbooks.  Comparing to our case, with &lt;span class="math"&gt;\(N=120\)&lt;/span&gt; we get good agreement,&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare proportion N=120.png"&gt;&lt;/p&gt;
&lt;p&gt;However, we start seeing differences where the textbook treatment isn't as informative as the methods we're using here.  For small samples, say &lt;span class="math"&gt;\(N=12\)&lt;/span&gt;, we see that the textbook approximation breaks down -- especially near the &lt;span class="math"&gt;\(f=0\)&lt;/span&gt; and &lt;span class="math"&gt;\(f=1\)&lt;/span&gt; boundaries, sometimes overestimating our certainty and sometimes underestimating it.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare proportion N=12.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice, however, that in the current approach we didn't need to change anything to get more robust answers.  Also notice that the current process for this problem is identical to the process for all of the other cases we've considered.  So what we are presenting, compared to the textbook treatments, is a more &lt;em&gt;unified&lt;/em&gt; approach which &lt;em&gt;generalizes&lt;/em&gt; to small samples giving a more &lt;em&gt;correct&lt;/em&gt; analysis for our data.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Stats 101 Examples with MCMC Part 2</title><link href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="alternate"></link><published>2021-04-28T00:00:00-04:00</published><updated>2021-04-28T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-04-28:/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/</id><summary type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  The previous in the series &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;can be found here&lt;/a&gt;.  In all of these posts I'm going to ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  The previous in the series &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;can be found here&lt;/a&gt;.  In all of these posts I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade&lt;/span&gt;
&lt;span class="err"&gt;pip install emcee&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, like &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;the last one&lt;/a&gt;, there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points we have, &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, consist of that true value with added independent, normally-distributed noise of &lt;em&gt;unknown&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined exactly as before,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
x_i &amp;amp;\sim&amp;amp; \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim&amp;amp; \text{Normal}(x_i-\mu,\sigma) \\
P(\{x_i\}|\mu,\sigma) &amp;amp;\sim&amp;amp; \prod_i \text{Normal}(x_i-\mu,\sigma) 
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
but with &lt;em&gt;unknown&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the location parameters, but &lt;a href="https://en.wikipedia.org/wiki/Jeffreys_prior"&gt;Jeffreys&lt;/a&gt; priors on &lt;em&gt;scale&lt;/em&gt; parameters, so we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and Jeffreys priors on &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="setting-up-the-problem"&gt;Setting up the problem&lt;/h2&gt;
&lt;p&gt;In this example, like &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/"&gt;the last one&lt;/a&gt;, there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, with normally-distributed noise but this time with unknown scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  The likelihood function is determined identically as before, &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
x_i &amp;amp;\sim&amp;amp; \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim&amp;amp; \text{Normal}(\mu,\sigma) 
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The prior for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, which we must now estimate, is taken to be a &lt;a href="https://en.wikipedia.org/wiki/Jeffreys_prior"&gt;Jeffreys prior&lt;/a&gt;.  This prior is uniform in &lt;span class="math"&gt;\(\log \sigma\)&lt;/span&gt; or, in other words, uniform in &lt;em&gt;scale&lt;/em&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# known œÉ&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lognormalpdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;œÉ&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;12.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MCMCModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="n"&gt;œÉ&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Jeffreys&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="running-mcmc-looking-at-the-results"&gt;Running MCMC, looking at the results&lt;/h2&gt;
&lt;p&gt;Now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_mcmc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_chains&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;Sampling Prior...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;0.27 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 1/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.63 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 2/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.79 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 3/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.47 s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/ŒºœÉ chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_distributions&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Œºt distribution.png"&gt;
&lt;img alt="" src="https://bblais.github.io/images/œÉ distribution.png"&gt;&lt;/p&gt;
&lt;h2 id="comparing-to-textbook-examples"&gt;Comparing to textbook examples&lt;/h2&gt;
&lt;p&gt;We compare to textbook solution for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (i.e. Student t-distribution),&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Œº&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ŒºÃÇ&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;œÉŒº&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ddof&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logtpdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ŒºÃÇ&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;œÉŒº&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare Œº t.png"&gt;&lt;/p&gt;
&lt;p&gt;and the textbook solution for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; (i.e. Chi-square),&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;œÉ&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;V&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;logp&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;V&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dx&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dx&lt;/span&gt;

&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare œÉ.png"&gt;&lt;/p&gt;
&lt;h2 id="tail-area-probabilities"&gt;Tail-area Probabilities&lt;/h2&gt;
&lt;p&gt;We can easily find the tail-area probability, the Bayesian equivalent to Student-t test, for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Œº&amp;gt;15&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;0.037066666666666664&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, the Bayesian equivalent to Chi-squared test, &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;œÉ&amp;lt;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;0.01985&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Stats 101 Examples with MCMC</title><link href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="alternate"></link><published>2021-04-15T00:00:00-04:00</published><updated>2021-04-15T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-04-15:/posts/2021/Apr/15/stats-101-examples-with-mcmc/</id><summary type="html">&lt;p&gt;I'd like to walk through some of the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples (e.g. estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; with known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, estimating a proportion, etc...) for which we have simple analytical solutions, but ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'd like to walk through some of the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples (e.g. estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; with known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, estimating a proportion, etc...) for which we have simple analytical solutions, but explore them with &lt;a href="https://www.youtube.com/watch?v=vTUwEu53uzs"&gt;MCMC&lt;/a&gt;.  The reason I want to do this is to introduce MCMC on simple cases, which are not much different than more complicated cases, but are better understood.  The second reason is to explore some cases which are nearly as easy, but are never covered in introductory textbooks.  In all of these I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade&lt;/span&gt;
&lt;span class="err"&gt;pip install emcee&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We'll start with the easiest example:  there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points we have, &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, consist of that true value with added independent, normally-distributed noise of &lt;em&gt;known&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined like&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
x_i &amp;amp;\sim&amp;amp; \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim&amp;amp; \text{Normal}(x_i-\mu,\sigma) \\
P(\{x_i\}|\mu,\sigma) &amp;amp;\sim&amp;amp; \prod_i \text{Normal}(x_i-\mu,\sigma) 
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the parameters, so we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# known œÉ&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lognormalpdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;œÉ&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;12.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;œÉ&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MCMCModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;lnlike&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;Œº&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_mcmc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_chains&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;Sampling Prior...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;0.21 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 1/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.23 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 2/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.24 s&lt;/span&gt;
&lt;span class="err"&gt;Running MCMC 3/3...&lt;/span&gt;
&lt;span class="err"&gt;Done.&lt;/span&gt;
&lt;span class="err"&gt;2.51 s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Œº chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_distributions&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Œº distribution.png"&gt;&lt;/p&gt;
&lt;p&gt;compare to textbook solution (i.e. Normal with deviation &lt;span class="math"&gt;\(\sigma/\sqrt{N}\)&lt;/span&gt;),&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Œº&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ŒºÃÇ&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;œÉŒº&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;œÉ&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lognormalpdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ŒºÃÇ&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;œÉŒº&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare Œº.png"&gt;&lt;/p&gt;
&lt;p&gt;and finally find a tail-area probability, the Bayesian equivalent to Z-test,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Œº&amp;gt;15&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;0.037066666666666664&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This simple example can be a template for more complex problems, which I will explore in later posts of this series.  What do you think?&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Probability - It's Not Just about the Math</title><link href="https://bblais.github.io/posts/2019/Jun/16/probability-its-not-just-about-the-math/" rel="alternate"></link><published>2019-06-16T00:00:00-04:00</published><updated>2019-06-16T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-06-16:/posts/2019/Jun/16/probability-its-not-just-about-the-math/</id><summary type="html">&lt;p&gt;When I first learned probability, I thought it was all about math and counting.  Then &lt;a href="https://bayes.wustl.edu/etj/etj.html"&gt;E. T. Jaynes&lt;/a&gt; showed me that probability forms the foundation of rationality itself.  Remarkably very ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I first learned probability, I thought it was all about math and counting.  Then &lt;a href="https://bayes.wustl.edu/etj/etj.html"&gt;E. T. Jaynes&lt;/a&gt; showed me that probability forms the foundation of rationality itself.  Remarkably very few axioms are needed to constrain the mathematical forms necessary for rational thought and the outcome turns out to be &lt;a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#Analytic_theory_of_probabilities"&gt;Laplace's original formulation&lt;/a&gt;.  What follows are the axioms so that you can appreciate them as much as I.  The paper &lt;em&gt;&lt;a href="https://bayes.wustl.edu/gregory/articles.pdf"&gt;From Laplace to Supernova SN1987A: Bayesian Inference In Astrophysics&lt;/a&gt;&lt;/em&gt; by Tom Loredo is an excellent and complete guide to this, including quantitative examples.  &lt;/p&gt;
&lt;p&gt;Jaynes prefers the word &lt;em&gt;desiderata&lt;/em&gt; for this list - a collection of things needed or wanted - but they function the same as &lt;em&gt;axioms&lt;/em&gt; of the analysis - unproven foundational statements as a basis for a derived system - and I prefer to use that terminology.  Whatever you'd like to name them, here they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Degrees of plausibility are represented by real numbers&lt;/strong&gt;.  This is a choice of practicality.  Perhaps there is some value in probability represented with &lt;em&gt;complex numbers&lt;/em&gt;, but so far no useful generalization has been found to my knowledge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qualitative consistency with common sense&lt;/strong&gt;.  Specifically, the system must be consistent with standard Boolean logic - the logic of syllogisms and deductive logic.  This is not a generic call to "&lt;em&gt;common sense&lt;/em&gt;" to base the system on, but rather the short-hand informal speech that Jaynes likes to use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Internal consistency&lt;/strong&gt;. If a conclusion can be reasoned out in more than one way then every possible way must yield the same result.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propriety&lt;/strong&gt;.  We must take into account all of the information provided that is relevant to the question.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent states consistency&lt;/strong&gt;.  Equivalent states of knowledge must be represented by equivalent plausibility assignments.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To quote Jaynes: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At this point, most students are surprised to learn that our search for desiderata is at an end. The above conditions, it turns out, uniquely determine the rules by which [we] must reason; i.e. there is only one set of mathematical operations for manipulating plausibilities which has all these properties.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From these axioms, we can derive,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the sum and product rules of probability, which describe the process of combining bits of knowledge&lt;/li&gt;
&lt;li&gt;Bayes Rule, which describes the structure of learning from evidence&lt;/li&gt;
&lt;li&gt;the marginalization rule, where model simplicity is attributed to fewer adjustable parameters&lt;/li&gt;
&lt;li&gt;deductive logic, the limit as the probabilities go to 0 and 1&lt;/li&gt;
&lt;li&gt;the dangers of either/or thinking&lt;/li&gt;
&lt;li&gt;and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An important consequence of the derivation is that &lt;em&gt;any&lt;/em&gt; system which disagrees with it must violate one or more of the axioms listed.  The entire approach shows that probability theory is far more fundamental than is typically appreciated.&lt;/p&gt;</content><category term="math"></category></entry><entry><title>An Interesting Twist on the Gambler's Fallacy</title><link href="https://bblais.github.io/posts/2019/Mar/24/an-interesting-twist-on-the-gamblers-fallacy/" rel="alternate"></link><published>2019-03-24T00:00:00-04:00</published><updated>2019-03-24T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-03-24:/posts/2019/Mar/24/an-interesting-twist-on-the-gamblers-fallacy/</id><summary type="html">&lt;p&gt;I was reminded of the &lt;a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy"&gt;Gambler's Fallacy&lt;/a&gt; in a &lt;a href="https://theness.com/neurologicablog/index.php/the-gamblers-fallacy/"&gt;recent post by Stephen Novella&lt;/a&gt; and thought of a twist on the fallacy that makes it a bit more subtle.  The ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was reminded of the &lt;a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy"&gt;Gambler's Fallacy&lt;/a&gt; in a &lt;a href="https://theness.com/neurologicablog/index.php/the-gamblers-fallacy/"&gt;recent post by Stephen Novella&lt;/a&gt; and thought of a twist on the fallacy that makes it a bit more subtle.  The fallacy is defined,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the mistaken belief that, if something happens more frequently than normal during a given period, it will happen less frequently in the future (or vice versa). In situations where the outcome being observed is truly random and consists of independent trials of a random process, this belief is false.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you observe 5 heads in a row, for example, then the probability of another heads is still &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt;. In other words it is not more (or less) likely to get a tails after a long string of heads in a row. There are however a couple of cases where this doesn't hold.  &lt;/p&gt;
&lt;h2 id="100-heads"&gt;100 heads&lt;/h2&gt;
&lt;p&gt;What happens if you observe someone flipping a coin and they achieve 100 heads in a row.  Would you really think that  the probability of another heads is still &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt;?  Is it even &lt;em&gt;rational&lt;/em&gt; to believe that?  At some point, you would question the &lt;em&gt;independence of trials&lt;/em&gt; assumption.  You might think that the coin is rigged (two heads?) or that the flipping process is rigged (spinning the coin like a frisbee instead of end-over-end motion) or something else.  The mathematics for it is pretty straightforward.  Assume you have two models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_1\)&lt;/span&gt;: the coin-flipping process is random, trials are independent, and the probability of each flip is &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt; for heads&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_2\)&lt;/span&gt;: the coin has two heads, so no matter what the flipping process is the probability of getting heads is &lt;span class="math"&gt;\(p=1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and our data is &lt;span class="math"&gt;\(m\)&lt;/span&gt; heads flips in a row&lt;/p&gt;
&lt;div class="math"&gt;$$
D \equiv \left\{\underbrace{H_1,H_2,H_3,\cdots,H_m}_{\mbox{$m$ flips}}\right\}
$$&lt;/div&gt;
&lt;p&gt;We are then interested in whether the &lt;span class="math"&gt;\(m+1\)&lt;/span&gt; flip is a heads, or &lt;span class="math"&gt;\(P(H_{m+1}|D)\)&lt;/span&gt;.  Since we have two possible models which could give this outcome, we break this probability into a sum for each,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(H_{m+1}|D) = P(H_{m+1}|D,M_1)P(M_1|D) + P(H_{m+1}|D,M_2)P(M_2|D)
$$&lt;/div&gt;
&lt;p&gt;Since in both models the history of flips makes no difference to the next flip we get&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
P(H_{m+1}|D,M_1) &amp;amp;=&amp;amp; P(H_{m+1}|M_1) = 0.5\\
P(H_{m+1}|D,M_2) &amp;amp;=&amp;amp; P(H_{m+1}|M_2) = 1
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;So, the bulk of the probability comes from the probability of each model given the data.  Applying Bayes' Rule, we get &lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
P(M_1|D) &amp;amp;\sim&amp;amp; P(D|M_1)P(M_1)=\frac{0.5^m \cdot P(M_1)}{T} \\
P(M_2|D) &amp;amp;\sim&amp;amp; P(D|M_2)P(M_2)=\frac{1^m \cdot P(M_2)}{T}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(T\)&lt;/span&gt; is the total probability of the data and the probabilities &lt;span class="math"&gt;\(P(M_1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(M_2)\)&lt;/span&gt; are the priors for Model 1 and Model 2, respectively.  &lt;/p&gt;
&lt;p&gt;We can start figuring out the prior probability by thinking in the following way.  Since we usually don't think a system is rigged unless we start seeing a strong pattern consistent with that, we expect that the prior for &lt;span class="math"&gt;\(M_2\)&lt;/span&gt; should be &lt;em&gt;much less&lt;/em&gt; than for &lt;span class="math"&gt;\(M_1\)&lt;/span&gt;.  Without loss of generality, we assign &lt;span class="math"&gt;\(P(M_1) = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(M_2) = 10^{-a}\)&lt;/span&gt;.  If &lt;span class="math"&gt;\(a=3\)&lt;/span&gt; then Model 2 is 1000x less likely (before the data) than Model 1,  if &lt;span class="math"&gt;\(a=6\)&lt;/span&gt; then Model 2 is 1 million times less likely, etc...  We  consider that the coin might not be independent when the probabilities for the two models -- after the data -- become comparable.  Each heads observation makes Model 1 a bit less likely and Model 2 a bit more so.  In a picture (for the case of &lt;span class="math"&gt;\(a=6\)&lt;/span&gt;) we have&lt;/p&gt;
&lt;p&gt;&lt;img alt="coin_flips_rigged.png" class="img-fluid" src="https://bblais.github.io/images/coin_flips_rigged.png"&gt;&lt;/p&gt;
&lt;p&gt;Mathematically we have&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(M_2|D)}{P(M_1|D)} = \frac{10^{-a}}{0.5^m} \gt 1
$$&lt;/div&gt;
&lt;p&gt;which yields the number of flips, &lt;span class="math"&gt;\(m\)&lt;/span&gt;, to overcome a prior against the two-headed coin, &lt;span class="math"&gt;\(a\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
m&amp;gt; \frac{a}{\log_{10} 2}
$$&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(a=6\)&lt;/span&gt; (or initially a million times less likely) then &lt;span class="math"&gt;\(m&amp;gt;19.9\)&lt;/span&gt; flips is needed to overcome that level of initial skepticism. &lt;/p&gt;
&lt;h2 id="alternating-results-or-quasi-random-processes"&gt;Alternating Results or Quasi-random processes&lt;/h2&gt;
&lt;p&gt;The same sort of analysis can occur if instead of a string of heads we observe a perfectly oscillating sequence (H,T,H,T,H,T,...).  This, of course, could be the result of a random process but if it continued long enough then even a low prior of a rigged system could be overcome.  &lt;/p&gt;
&lt;p&gt;The same could occur if you happen to be observing a &lt;a href="https://en.wikipedia.org/wiki/Sobol_sequence"&gt;Sobol Sequence&lt;/a&gt; or some other &lt;a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence"&gt;quasi-random processes&lt;/a&gt;. These sequences are &lt;em&gt;designed&lt;/em&gt; to be non-independent in order to be more likely to fill the space of numbers evenly.  Thus, if there is a sequence of 5 heads in a row, then under a quasi-random process a tails is in fact &lt;em&gt;more likely&lt;/em&gt; in these sequences as the "fallacy" warns us against.  These processes have some use in sampling for simulations where you want to make sure to cover all the the values of the parameter space evenly and efficiently.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I am not arguing that the Gambler's Fallacy is wrong, nor am I arguing that we should think every case is rigged.  I am further not suggesting that people are actually good at these problems - they aren't.  We see patterns in random sequences and events all of the time, and perhaps we should be a bit more reticent to ascribe non-randomness to processes for which we have no evidence of a non-random influence.  In other words, our priors for a rigged system should generally be quite low.  However, there can be cases where the system &lt;em&gt;is rigged&lt;/em&gt; and we should be open to that possibility as well.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>If at first you don't succeed (for 50 times) you might reconsider</title><link href="https://bblais.github.io/posts/2019/Jan/29/if-at-first-you-dont-succeed-for-50-times-you-might-reconsider/" rel="alternate"></link><published>2019-01-29T00:00:00-05:00</published><updated>2019-01-29T00:00:00-05:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-01-29:/posts/2019/Jan/29/if-at-first-you-dont-succeed-for-50-times-you-might-reconsider/</id><summary type="html">&lt;p&gt;Here's a &lt;a href="https://www.quora.com/If-there-is-a-25-chance-of-something-what-is-the-probability-that-you-fail-to-succeed-50-times-in-a-row-before-you-finally-do-succeed/answer/Brian-Blais"&gt;straightforward problem&lt;/a&gt; from Quora - probably found in a textbook - but I try to find a new direction to make it more interesting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If there is a 25% chance ‚Ä¶&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;Here's a &lt;a href="https://www.quora.com/If-there-is-a-25-chance-of-something-what-is-the-probability-that-you-fail-to-succeed-50-times-in-a-row-before-you-finally-do-succeed/answer/Brian-Blais"&gt;straightforward problem&lt;/a&gt; from Quora - probably found in a textbook - but I try to find a new direction to make it more interesting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If there is a 25% chance of something, what is the probability that you fail to succeed 50 times in a row before you finally do succeed?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem, as stated, gives you a very small probability of 50 failures followed by a success.  As stated, this probability is simply&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(S_{51}, F_{1,2,\cdots,50}) = 0.75^{50} \times 0.25 \sim 10^{-7}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, we rarely know that an event has &lt;em&gt;exactly&lt;/em&gt; a probability of 25% - this is usually just a hunch.  Or possibly we &lt;em&gt;believe&lt;/em&gt; the probability to be 25% but in fact it‚Äôs actually 0% or in other words (unknowingly) impossible - the claim itself can be wrong.  Further, the way the question is worded almost implies that you're trying to decide to abandon...or not...an action after 50 failures.  This is a different question than the original, I admit, but much closer to what one might expect in a more realistic scenario.&lt;/p&gt;
&lt;p&gt;Pursuing this adjusted question, say we have the following two possibilities: the original claim that &lt;span class="math"&gt;\(p_{\rm event}=0.25\)&lt;/span&gt; or the claim is false and (for example) &lt;span class="math"&gt;\(p_{\rm event}=0\)&lt;/span&gt;.  Let‚Äôs further assume that we are quite sure of the original claim, at level of 99.99%.  With the new question, we are interested in the probability of a success on try 51 after 50 previous failures, which is now a combination of the result of the claim possibly being true and the claim possibly being false.  I will outline the process for &lt;span class="math"&gt;\(m\)&lt;/span&gt; failures instead of 50, so that we can discuss it in a more interesting way.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(S_{51}| F_{1,2,\cdots,50}) = P(S_{51}| F_{1,2,\cdots,m}, {\rm True\ Claim})P({\rm True\ Claim}|F_{1,2,\cdots,m}) + P(S_{51}| F_{1,2,\cdots,m}, {\rm False\ Claim})P({\rm False\ Claim}|F_{1,2,\cdots,m})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key part of this calculation is the comparison of the two possibilities (i.e. the claim is true or the claim is false) after observing 50 failures. We can apply Bayes Rule, &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P({\rm True\ Claim}|F_{1,2,\cdots,m}) \sim P(F_{1,2,\cdots,m}| {\rm True\ Claim})\cdot P({\rm True\ Claim}) \sim 0.75^m \cdot 0.9999\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P({\rm False\ Claim}|F_{1,2,\cdots,m}) \sim P(F_{1,2,\cdots,m}| {\rm False\ Claim})\cdot P({\rm False\ Claim})\sim 1 \cdot 0.0001\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After only 32 failures these two probabilities are equal, or in other words, it's even money that the action is even &lt;em&gt;possible&lt;/em&gt;.  After 50 failures, the odds for the original claim are around 180:1 against.  Clearly we are facing the definition of insanity&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; at this point.&lt;/p&gt;
&lt;p&gt;Essentially, because 50 failures is so unlikely for a 25% chance event or action, other possibilities intrude - such as "success &lt;em&gt;at all&lt;/em&gt; is impossible".  There can, of course, be other possibilities that come to mind after many failures beyond the one I explore here.  However, the point that I want to stress is that one should think a little more deeply when faced even with such "simple" problems.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;doing the same thing over and over expecting a different result.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Cool theorem - angle bisector</title><link href="https://bblais.github.io/posts/2018/Jan/29/cool-theorem-angle-bisector/" rel="alternate"></link><published>2018-01-29T00:00:00-05:00</published><updated>2018-01-29T00:00:00-05:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2018-01-29:/posts/2018/Jan/29/cool-theorem-angle-bisector/</id><summary type="html">&lt;p&gt;I was helping my daughter with geometry the other day and &lt;a href="https://en.wikipedia.org/wiki/Angle_bisector_theorem"&gt;this cute theorem popped up in the examples&lt;/a&gt;.  For some reason I don't recall ever seeing it before, and ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was helping my daughter with geometry the other day and &lt;a href="https://en.wikipedia.org/wiki/Angle_bisector_theorem"&gt;this cute theorem popped up in the examples&lt;/a&gt;.  For some reason I don't recall ever seeing it before, and it struck me as somehow particularly elegant.  I have used (many times) the specific - &lt;a href="http://ceemrr.com/Geometry1/RightTriangles/RightTriangles4.html"&gt;more limited version with isosceles triangles&lt;/a&gt; - but to see the generalization was just neat.&lt;/p&gt;
&lt;p&gt;It makes me wonder also why I find enjoyment in something this simple and technical.  &lt;/p&gt;</content><category term="math"></category></entry></feed>