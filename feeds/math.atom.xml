<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>bblais on the web - math</title><link href="https://bblais.github.io/" rel="alternate"></link><link href="https://bblais.github.io/feeds/math.atom.xml" rel="self"></link><id>https://bblais.github.io/</id><updated>2025-08-03T00:00:00-04:00</updated><entry><title>The Nines Deck and Scientific Reasoning</title><link href="https://bblais.github.io/posts/2025/Aug/03/the-nines-deck-and-scientific-reasoning/" rel="alternate"></link><published>2025-08-03T00:00:00-04:00</published><updated>2025-08-03T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2025-08-03:/posts/2025/Aug/03/the-nines-deck-and-scientific-reasoning/</id><summary type="html">&lt;p&gt;I wrote about the &lt;a href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="nofollow"&gt;Nines Deck here&lt;/a&gt; as an example of multiple model comparison.  Here I want to extend the example to explore more about scientific reasoning.&lt;/p&gt;
&lt;p&gt;I ended my …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I wrote about the &lt;a href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="nofollow"&gt;Nines Deck here&lt;/a&gt; as an example of multiple model comparison.  Here I want to extend the example to explore more about scientific reasoning.&lt;/p&gt;
&lt;p&gt;I ended my post with the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The non-monotonic effect is also why, when a psychic demonstrates some amazing feat of apparent mind-reading, they still won't be believed immediately. This is because many rare and unconsidered explanations will rise from low prior status to dominate.  These low prior models still have a higher prior value than the ESP model, but are low enough to not be considered immediately.  The next step would be to modify the data collection process to potentially eliminate them.  In other words, to design an experiment to rule out these alternate explanations.   This is the process of science -- designing experiments to make these low prior models less likely in the hope of increasing the probability of the model under consideration.&lt;br&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I do find that proponents of pseudoscientific claims get frustrated with this process.  They don't understand how, once the high-prior alternative explanations have been removed that their magical claim is not immediately believed.  Instead, new models never discussed rise to the surface and have to be dealt with.  Accusations of naturalistic or other bias against their claims come out, the investigator is accused of being "too skeptical", of never being convinced no matter what the evidence.  This comes from a lack of understanding of how probability actually works, usually because the versions of these same calculations are done with binary models, lack of imagination, and no attempt at verifying the numbers.&lt;/p&gt;
&lt;p&gt;So I have been trying to think of good analogies to the process of ruling out high-prior explanations which seem to point to extremely low-prior explanations, but along the way raising some somewhat low-prior explanations.  Going back to my &lt;a href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="nofollow"&gt;High-Low-Nines&lt;/a&gt; deck example, I'll draw out the analogy. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The High and Low deck explanations are mundane&lt;/li&gt;
&lt;li&gt;The Nines deck is an extremely low-prior explanation -- like the ESP model, or the &lt;a href="https://bblais.github.io/posts/2025/Jul/28/bayesian-analysis-and-its-misuses/" rel="nofollow"&gt;Resurrection model described by apologists&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The string of &lt;span class="math"&gt;\(m\)&lt;/span&gt; 9's in a row is a string of data all consistent with the low-prior explanation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The way that apologists would see it, we have a lot of data which is consistent with their explanation.  They rule out some of the simple alternatives, which leaves their preferred explanation, so you should believe their explanation.  That's like the figure in end of my post on the  &lt;a href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="nofollow"&gt;High-Low-Nines&lt;/a&gt; example. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Pasted image 20250803073308.png" src="https://bblais.github.io/images/Pasted image 20250803073308.png"&gt;&lt;/p&gt;
&lt;p&gt;I want to comment that the &lt;a href="https://www.inspiringphilosophy.com/blog/paulogiaits-time-to-stop" rel="nofollow"&gt;typical formulation of Bayes theorem&lt;/a&gt; from the apologists is in the odds-form -- a ratio of the probabilities of two models, or rather, one model and it's negation.  However, the result just shown and the entire discussion below is either impossible to see in the odds form or is much less clear.  As such, the thinking of the apologist is restricted to binary comparisons and the complexity of actual rational inference is lost to them.&lt;/p&gt;
&lt;p&gt;Now one might quibble about whether the apologist has the necessary amount of data to overcome the prior, but let's leave that for now.  In my original post, after a long string of 9's one can posit a few explanations.  There could be the very-rare Nines deck, or we could be looking at the High or Low deck still but that the shuffling wasn't done properly.  It may have &lt;em&gt;looked&lt;/em&gt; like the process shuffled, or we might not have seen it and were just &lt;em&gt;told&lt;/em&gt; it was reshuffled.  So another set of models could be an unshuffled Low deck or unshuffled High deck -- we keep taking the top card, which happens to be the same card every time.  While this a more nuanced explanation it does not introduce another kind of deck.  As such, it has a very low prior at the start but higher than the Nines deck explanation.  What would this look like?  In the following figure I am having the unshuffled versions of the Decks have a prior of 1/1000 -- still very small, but larger than the prior for the Nines.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Pasted image 20250803073815.png" src="https://bblais.github.io/images/Pasted image 20250803073815.png"&gt;&lt;/p&gt;
&lt;p&gt;What we can see is that the continued stream of 9's is consistent with the more mundane unshuffled models and thus the rare Nines explanation never rises to believability -- and never will.  Now the apologist-scientist seeing this, and trying to rescue the Nines model, would propose an experiment.  After about 10 draws it's pretty clear nothing is happening, so the apologist-scientist decides to change the procedure a bit (i.e. the experiment).  While not having access to the shuffling (or not) mechanism, the apologist-scientist decides to draw from the bottom rather than from the top.  This at least makes the possibility of falsifying the Nines model, if anything other than a 9 appears.  If a 9 appears, it should make the Nines model more likely and the other models should go down (at least that's the intuition).  Here's the result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pasted image 20250803074336.png" src="https://bblais.github.io/images/Pasted image 20250803074336.png"&gt;&lt;/p&gt;
&lt;p&gt;While drawing from the bottom and seeing another sequence of 9's does increase the probability of the Nines deck, the unshuffled High Deck probability goes up!  Most of the loss of the unshuffled Low Deck probability gets reassigned to the unshuffled High Deck with only a modest increase in the Nines deck probability.  This violates some of our intuition, but that is one of the values of doing the probability analysis -- it allows us to &lt;em&gt;learn&lt;/em&gt; a more sophisticated intuition.  Also, it should remind us that testing results, being quantitative, and having more than one way to look at a problem are all invaluable.  &lt;/p&gt;
&lt;p&gt;One might critique the scientist saying that, if you don't have access to the shuffling mechanism, then don't just draw from the bottom, draw from a random part of the deck.  In some cases in science, there are some experiments we don't have access to, and can only do a limited number of types of measurements.  In history, we don't have a time machine, and sometimes the data just isn't enough to answer certain questions.  We just don't have access or control of the data to address them.  That's why the conclusions in history are always much more uncertain and tentative than in the sciences. &lt;/p&gt;
&lt;p&gt;As a reminder, none of these effects would be observable to the apologist who insists on writing the math in odds-form.  &lt;/p&gt;
&lt;p&gt;It is my hope that this simple example can structure a conversation around many topics, and get us away from simplistic Bayesian arguments that use the odds-form. I find that this toy model of the &lt;a href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="nofollow"&gt;High-Low-Nines&lt;/a&gt; deck allows us to play with our intuitions, explore analogies in an example which has some nice parallels with scientific reasoning.  Are there other ways of framing this analogy that are more clear and help with communication?  &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>The Nines Deck and Non-Monotonic Probability Changes</title><link href="https://bblais.github.io/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/" rel="alternate"></link><published>2025-08-02T00:00:00-04:00</published><updated>2025-08-02T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2025-08-02:/posts/2025/Aug/02/the-nines-deck-and-non-monotonic-probability-changes/</id><summary type="html">&lt;p&gt;I realized that I don't have a blog post about my favorite simple probability example, the High-Low Deck Game.  I've written about it in &lt;a href="https://bblais.github.io/posts/2019/Jan/14/stats-for-everyone/" rel="nofollow"&gt;Statistical Inference for Everyone&lt;/a&gt; but I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I realized that I don't have a blog post about my favorite simple probability example, the High-Low Deck Game.  I've written about it in &lt;a href="https://bblais.github.io/posts/2019/Jan/14/stats-for-everyone/" rel="nofollow"&gt;Statistical Inference for Everyone&lt;/a&gt; but I think it is useful to have it written here, demonstrate a notational difference from many statistics textbooks, and talk about some of the interesting conclusions.  &lt;/p&gt;
&lt;h2 id="the-game"&gt;The Game&lt;/h2&gt;
&lt;p&gt;We start with two atypical decks of cards called the Low Deck and the High Deck,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pasted image 20250801201623.png" src="https://bblais.github.io/images/Pasted image 20250801201623.png"&gt;&lt;/p&gt;
&lt;p&gt;You'll notice that both decks have 55 cards, and that the Low Deck contains many more low cards while the High Deck contains many more high cards.  The game is played like,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You're handed one of the two decks, but you don't know which.  &lt;/li&gt;
&lt;li&gt;You draw the top card and note the value.  &lt;/li&gt;
&lt;li&gt;You then replace that card and &lt;em&gt;reshuffle the deck&lt;/em&gt; (aka. sample with replacement). This version of the game with reshuffling will help in cases where we want to draw many times and not worry about running out of cards or running out of any particular number.   &lt;/li&gt;
&lt;li&gt;Repeat this procedure of drawing, noting, and reshuffling for as many turns as you need.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal is to determine which of the two decks (Low or High) you are in fact holding in your hand.  In practice, we are calculating the probability we are holding the Low Deck given the data, &lt;span class="math"&gt;\(P(L|\text{data})\)&lt;/span&gt;, and the High Deck given the data, &lt;span class="math"&gt;\(P(H|\text{data})\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;If you're impatient, you can jump down to &lt;a href="#the-final-result" rel="nofollow"&gt;the fInal result&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-does-our-intuition-say"&gt;What does our intuition say?&lt;/h2&gt;
&lt;p&gt;We start by exploring our intuitions, before we do anything mathematically.  Thus, we are in a position to check to see if the math is reasonable before we use the same math in areas where our intuition is not as strong.  Imagine we draw only one card, and it is a 9.  Intuition suggests that this constitutes reasonably strong evidence toward the belief that we're holding the High Deck.  If we then (as the procedure states) place the 9 back in the deck, reshuffle, and then draw a 7 we can be more strongly convinced that we are holding the High Deck.  Repeating the reshuffle, and then drawing a 3 would make us a little less confident in this conclusion, but still quite certain.  In this way we can sense how drawing different cards pushes our belief around, depending on how often that card comes up in the different decks.&lt;/p&gt;
&lt;h2 id="before-the-data-the-prior"&gt;Before the data - the prior&lt;/h2&gt;
&lt;p&gt;Before we take any data, we need to quantify our state of knowledge concerning all the models that we are considering.  In this case it is quite simple, because there are two models (High Deck and Low Deck), and we have been given no information about whether either is more common.  With no such information, it is equivalent to a coin flip - we assign equal probabilities to both models &lt;em&gt;before&lt;/em&gt; we see data, also known as the prior probabilities.  &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
P(H) =&amp;amp; 0.5 \\
P(L) =&amp;amp; 0.5
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Surely this assessment will change &lt;em&gt;after&lt;/em&gt; we see data, but that is the rest of the problem.&lt;/p&gt;
&lt;h2 id="the-easy-question-the-likelihood"&gt;The "easy" question - the likelihood&lt;/h2&gt;
&lt;p&gt;Although our ultimate goal is to infer the type of deck from the cards that we draw from it, we can start looking at an easier part of this question. This serves as a first step toward the more challenging, and interesting goal.  That question is the following, 
&lt;em&gt;What is the probability of drawing a 9, given that we know that we're holding the High Deck?&lt;/em&gt;  This  question is written mathematically as&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P({\rm data}=9|H)
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\({\rm data}=9\)&lt;/span&gt; means that we have observed (i.e. drawn) one 9.  This question is "easy" in the sense that it is simply related to the properties of the High Deck: the number of 9's and total number of cards.  If you know that you have the High Deck, then you know there are nine 9's in that deck out of 55 cards.  Thus, we have the probability of drawing one 9, given that we are holding the High Deck, &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P({\rm data}=9|H)=\frac{9}{55}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
We give this the name &lt;em&gt;likelihood&lt;/em&gt;, and is simply the probability that the data could be the result of a known model.  &lt;/p&gt;
&lt;h2 id="applying-the-bayes-recipe"&gt;Applying the Bayes' recipe&lt;/h2&gt;
&lt;p&gt;Now that we have our intuition, and we have the likelihoods, we can address the math.  The two models are:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
H&amp;amp;\equiv \text{"We're holding the High Deck"}\\
L&amp;amp;\equiv \text{"We're holding the Low Deck"}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
and the initial data is&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \text{"We've drawn one card, and it is a 9"}\\
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
We are look for the two probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9) \\
P(L|{\rm data}=9) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
which are related to the &lt;em&gt;prior&lt;/em&gt; and the  &lt;em&gt;likelihood&lt;/em&gt; via Bayes' Rule &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9)&amp;amp;=\frac{P({\rm data}=9|H)P(H)}{P({\rm data}=9)}\\
P(L|{\rm data}=9)&amp;amp;=\frac{P({\rm data}=9|L)P(L)}{P({\rm data}=9)} 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
To calculate actual numbers, we apply the Bayes' Recipe to this problem,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the prior probabilities for the models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H) &amp;amp;= 0.5 \\
P(L) &amp;amp;= 0.5
\end{aligned}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Write the top of Bayes' Rule for all models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9)&amp;amp;\sim P({\rm data}=9|H)P(H) \\
P(L|{\rm data}=9)&amp;amp;\sim P({\rm data}=9|L)P(L) 
\end{aligned}
$$&lt;/div&gt;
where we are using the symbol &lt;span class="math"&gt;\(\sim\)&lt;/span&gt; to denote &lt;em&gt;proportionality&lt;/em&gt; or &lt;em&gt;related to&lt;/em&gt;.  Essentially, by calculating the top of Bayes' Rule first, the numbers are not &lt;em&gt;equal&lt;/em&gt; to the final (i.e. posterior) probabilities but must be rescaled to make sure that they add up to 1.  This is done in the final step.  Up until that rescaling, we use the symbol &lt;span class="math"&gt;\(\sim\)&lt;/span&gt; and think of it as &lt;em&gt;related to&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Put in the likelihood and prior values
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9)&amp;amp;\sim \frac{9}{55}\times 0.5 =0.082 \\
P(L|{\rm data}=9)&amp;amp;\sim \frac{2}{55}\times 0.5 =0.018 
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Add these values for all models
&lt;div class="math"&gt;$$\begin{aligned}
T=0.082+0.018 = 0.1
\end{aligned}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Divide each of the values by this sum, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, to get the final probabilities&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data})=0.082/0.1 = 0.82\\
P(L|{\rm data})=0.018 /0.1 = 0.18
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
From which we can conclude that drawing a 9 does indeed constitute reasonably strong evidence toward the belief that we're holding the High Deck - the probability of us holding the High Deck started at 0.5 and given the data is now 0.82.&lt;/p&gt;
&lt;h2 id="drawing-the-next-card"&gt;Drawing the next card&lt;/h2&gt;
&lt;p&gt;So, when we draw a 7 next (after reshuffling), our intuition suggests that we'd be more confident that we're holding the High Deck.  Repeating our recipe we have&lt;/p&gt;
&lt;p&gt;The two models are:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
H&amp;amp;\equiv \text{"We're holding the High Deck"}\\
L&amp;amp;\equiv \text{"We're holding the Low Deck"}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
and data is &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \left\{\text{"We've drawn one card, and it is a 9, replaced and}\right.\\
&amp;amp;\left.\text{reshuffled, and then drawn a 7"}\right\}\\
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
We are looking for the two probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7) \\
P(L|{\rm data}=9\text{ then a 7}) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;which are related to the &lt;em&gt;prior&lt;/em&gt; and the &lt;em&gt;likelihood&lt;/em&gt; via Bayes' Rule:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)&amp;amp;=\frac{P({\rm data}=9\text{ then a }7|H)P(H)}{P({\rm data}=9\text{ then a }7)}\\
P(L|{\rm data}=9\text{ then a }7)&amp;amp;=\frac{P({\rm data}=9\text{ then a }7|L)P(L)}{P({\rm data}=9\text{ then a }7)} 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;To calculate actual numbers, we apply the Bayes' recipe to this problem,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the prior probabilities for the models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H) &amp;amp;= 0.5 \\
P(L) &amp;amp;= 0.5
\end{aligned}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Write the top of Bayes' Rule for all models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)&amp;amp;\sim P({\rm data}=9\text{ then a }7|H)P(H) \\
P(L|{\rm data}=9\text{ then a }7)&amp;amp;\sim P({\rm data}=9\text{ then a }7|L)P(L) 
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Put in the likelihood and prior values
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)&amp;amp;\sim  \frac{9}{55}\times\frac{7}{55}\times 0.5 =0.0104 \\
P(L|{\rm data}=9\text{ then a }7)&amp;amp;\sim  \frac{2}{55}\times\frac{4}{55}\times 0.5 =0.0013 
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Add these values for all models
&lt;div class="math"&gt;$$\begin{aligned}
T=0.0104+0.0013 = 0.0117
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Divide each of the values by this sum, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, to get the final probabilities
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)=0.0104/0.0117 =0.889 \\
P(L|{\rm data}=9\text{ then a }7)=0.0013/0.0117 =0.111 
\end{aligned}
$$&lt;/div&gt;
which again matches our intuition -- we're more confident that we're holding the High Deck, now with probability 0.889 increased from 0.82 when we just observed the 9 and from 0.5 before we drew any cards.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="prior-information-or-not"&gt;Prior information or not?&lt;/h2&gt;
&lt;p&gt;In the above example, we started with a prior probability of holding the High Deck at &lt;span class="math"&gt;\(P(H)=0.5\)&lt;/span&gt;, because we had no information other than that there were two possibilities.  We then observed a 9, and updated the probability to 0.82, and then observed a 7, and further updated the probability to 0.889 - making it more likely that we were holding the High Deck.  &lt;/p&gt;
&lt;p&gt;One of the basic tenets of probability theory is that if there is more than one way to arrive at an answer, one should arrive at the same answer.  In the above, we calculated the probability of holding the High Deck given the observed data&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \left\{\text{"We've drawn one card, and it is a 9, replaced and}\right.\\
&amp;amp;\left.\text{reshuffled, and then we've drawn a 7"}\right\}\\
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
and prior information&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm prior}&amp;amp;\equiv \left\{\text{"We know there are only two decks."}\right\}\\
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;This is entirely identical to having the following &lt;em&gt;prior&lt;/em&gt; information:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm prior}&amp;amp;\equiv \left\{\text{"We know there are only two decks, and then}\right.\\
&amp;amp;\left.\text{we draw one card and it is a 9, replace it and reshuffle."}\right\}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;and observed data:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \{\text{"We've drawn one card and it is a 7"}\}.
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;As such, these two problems must yield identical answers.  &lt;/p&gt;
&lt;p&gt;Mathematically, we apply the Bayes' recipe, but with the different prior information&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the prior probabilities for the models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H,9) &amp;amp;= 0.82 \\
P(L,9) &amp;amp;= 0.18
\end{aligned}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Write the top of Bayes' Rule for all models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)&amp;amp;\sim P({\rm data}=7|H)P(H,9) \\
P(L|{\rm data}=9\text{ then a }7)&amp;amp;\sim P({\rm data}=7|L)P(L,9) 
\end{aligned}
$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Put in the likelihood and prior values
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)&amp;amp;\sim \frac{7}{55}\times 0.82 =0.104 \\
P(L|{\rm data}=9\text{ then a }7)&amp;amp;\sim \frac{4}{55}\times 0.18 =0.013
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Add these values for all models
&lt;div class="math"&gt;$$\begin{aligned}
T=0.104+0.013 = 0.117
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Divide each of the values by this sum, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, to get the final probabilities&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=9\text{ then a }7)=0.104/0.117 =0.889 \\
P(L|{\rm data}=9\text{ then a }7)=0.013/0.117 =0.111 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;yielding the same result.&lt;/p&gt;
&lt;p&gt;In other words our  updated probabilities from the first draw can be seen as our &lt;em&gt;prior&lt;/em&gt; probabilities for the subsequent draws.  Thus, Bayes' Rule describes how we update our knowledge with new evidence.  We can calculate these probabilities all at once with all the data, or step-by-step as the data come in -- the results must be identical.&lt;/p&gt;
&lt;h2 id="multiple-hypotheses"&gt;Multiple Hypotheses&lt;/h2&gt;
&lt;p&gt;This is where the notation we've been following really shines.  In other formulations, such as the odds form of Bayes theorem, moving beyond two models/hypotheses is cumbersome at best and impossible at worst.  We'll slowly walk into the multiple models, but for now let's assume we are either holding the High or Low Deck, and we observe this data:&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \left\{\text{"We've drawn one card, and it is a 9, replaced and}\right.\\
&amp;amp;\left.\text{reshuffled, and then drawn another 9, replaced and"}\right.\\
&amp;amp;\left.\text{reshuffled, }\cdots \text{, five times in a row."}\right\}\\
&amp;amp;\equiv\{\underbrace{9,9,9,9,9}_{5 \text{ times}}\}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Performing the same calculations, we get&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the prior probabilities for the models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H) &amp;amp;= 0.5 \\
P(L) &amp;amp;= 0.5
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Write the top of Bayes' Rule for all models being considered
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=5\text{ 9's in a row})&amp;amp;\sim P({\rm data}=5\text{ 9's in a row}|H)P(H) \\
P(L|{\rm data}=5\text{ 9's in a row})&amp;amp;\sim P({\rm data}=5\text{ 9's in a row}|L)P(L) 
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Put in the likelihood and prior values
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=5\text{ 9's in a row})&amp;amp;\sim\underbrace{\frac{9}{55}\times\frac{9}{55}\cdots\frac{9}{55}}_{5\text{ times}} \times P(H)\\
&amp;amp;\sim \left(\frac{9}{55}\right)^{5}\times 0.5 \\
&amp;amp;=0.0000587\\
P(L|{\rm data}=5\text{ 9's in a row})&amp;amp;\sim \left(\frac{2}{55}\right)^{5}\times 0.5\\
&amp;amp;=0.0000000318
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Add these values for all models
&lt;div class="math"&gt;$$\begin{aligned}
T=0.0000587+0.0000000318 = 0.0000587318
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Divide each of the values by this sum, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, to get the final probabilities
&lt;div class="math"&gt;$$\begin{aligned}
P(H|{\rm data}=5\text{ 9's in a row})&amp;amp;= \frac{0.0000587}{0.0000587318}=0.99946\\
P(L|{\rm data}=5\text{ 9's in a row})&amp;amp;= \frac{0.0000000318}{0.0000587318}=0.00054
\end{aligned}$$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;which is &lt;em&gt;fantastically&lt;/em&gt; on the side of the high deck.&lt;/p&gt;
&lt;p&gt;What about this data?
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \left\{\text{"We've drawn one card, and it is a 9, replaced and}\right.\\
&amp;amp;\left.\text{reshuffled, and then drawn another 9, replaced and"}\right.\\
&amp;amp;\left.\text{reshuffled, }\cdots \text{, fifteen times in a row."}\right\}\\
&amp;amp;\equiv\{\underbrace{9,9,9,9,\cdots,9,9,9}_{15 \text{ times}}\}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Blindly following the procedure above gets us &lt;span class="math"&gt;\(P(H|{\rm data})=0.999999999841\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;We might start getting suspicious in this situation, because even though the data of a string of 9's is highly expected on the High Deck, compared to the Low Deck, we should start seeing other cards as well.  We might start considering that the information that we've been given is incorrect -- that there are more decks than the High and the Low, or someone isn't actually reshuffling properly.  These are other models that can explain the data, but would have to be a-priori much lower for us to not have considered them before now.   As an example, let's introduce a third model -- the Nines Deck -- and follow the procedure again.  We'll generalize to an unspecified number of &lt;span class="math"&gt;\(m\)&lt;/span&gt; 9's, so that we can easily explore all possibilities.  &lt;/p&gt;
&lt;p&gt;What is interesting here is that once we admit that there are many possible models we could consider, we realize that we have these models in our head all the time, or we construct them as we need them.  Every model comparison is a multiple model comparison, with most of the models with very low prior probabilities that our brain naturally suppresses until needed.  Mathematically, we need to unsuppress them as needed, but their probability will have to be rescued with data. &lt;/p&gt;
&lt;p&gt;Let's say that we assign the prior probability for the Nines deck to be a one in a million.  To make all the prior probabilities add up to 1, the prior probabilities for the High and Low Deck must be a little less than 0.5.  After that, we simply apply the Bayes' Recipe as before&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm data}&amp;amp;\equiv \left\{\text{"We've drawn one card, and it is a 9, replaced and}\right.\\
&amp;amp;\left.\text{reshuffled, and then drawn another 9, replaced and"}\right.\\
&amp;amp;\left.\text{reshuffled, }\cdots \text{, $m$ times in a row."}\right\}\\
&amp;amp;\equiv\{\underbrace{9,9,9,\cdots,9,9}_{m \text{ times}}\}\\
\end{aligned}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\begin{aligned}
{\rm prior}&amp;amp;\equiv \left\{\text{"We know there are three decks: High, Low, and Nines"}\right\}\\
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
1. Specify the prior probabilities for the models being considered
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(N)&amp;amp;= \frac{1}{1,000,000}=0.000001 \\
P(H) &amp;amp;= 0.4999995 \\
P(L) &amp;amp;= 0.4999995
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
2. Write the top of Bayes' Rule for all models being considered
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(N|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim P({\rm data}=m\text{ 9's in a row}|N)P(N) \\
P(H|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim P({\rm data}=m\text{ 9's in a row}|H)P(H) \\
P(L|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim P({\rm data}=m\text{ 9's in a row}|L)P(L) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
3. Put in the likelihood and prior values
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
P(N|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim 1\times P(N)=0.000001\\
P(H|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim \underbrace{\frac{9}{55}\times\frac{9}{55}\cdots\frac{9}{55}}_{m\text{ times}} \times P(H)\\
&amp;amp;\sim  \left(\frac{9}{55}\right)^{m}\times 0.4999995 \\
P(L|{\rm data}=m\text{ 9's in a row})&amp;amp;\sim  \left(\frac{2}{55}\right)^{m}\times 0.4999995
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
4. Add these values for all models
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}
T=0.000001+\left(\frac{9}{55}\right)^{m}\times 0.4999995+\left(\frac{2}{55}\right)^{m}\times 0.4999995
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
5. Divide each of the values by this sum, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, to get the final probabilities.  This step is easiest done in a table or, even better, a picture, as in the following.&lt;/p&gt;
&lt;h2 id="the-final-result"&gt;The Final Result&lt;/h2&gt;
&lt;p&gt;&lt;img alt="nines_HLN.png" src="https://bblais.github.io/images/nines_HLN.png"&gt;&lt;/p&gt;
&lt;p&gt;There are a number of interesting observations about this figure.  When &lt;span class="math"&gt;\(m=0\)&lt;/span&gt;, we get the (nearly) 50/50 split between High and Low decks -- before we draw any data, this is the prior.  As we draw 9's, our confidence that we're holding the High Deck goes up, at the expense of our confidence that we're holding the Low Deck.  At a certain point (around six 9's in our example), our confidence in the High Deck starts to drop.  We become more confident that something odd is happening, and our previously ignored model of the Nines deck becomes more likely.  Eventually, this new model is the one in which we are the most confident.  &lt;/p&gt;
&lt;p&gt;This is an example of a non-monotonic probability change -- drawing the same data sometimes increases the probability of a model and sometimes decreases that very same model.  It all depends on the alternatives being considered.  I suspect that the non-monotonic effect only occurs when one considers more than two models.  As such, the standard way of presenting Bayesian analysis -- either using the full equation, or by looking at odds ratios -- never see this effect and the resulting analysis can be somewhat naive.&lt;/p&gt;
&lt;p&gt;The non-monotonic effect is also why, when a psychic demonstrates some amazing feat of apparent mind-reading, they still won't be believed immediately. This is because many rare and unconsidered explanations will rise from low prior status to dominate.  These low prior models still have a higher prior value than the ESP model, but are low enough to not be considered immediately.  The next step would be to modify the data collection process to potentially eliminate them.  In other words, to design an experiment to rule out these alternate explanations.   This is the process of science -- designing experiments to make these low prior models less likely in the hope of increasing the probability of the model under consideration.  &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Stats 101 Examples with MCMC Part 4</title><link href="https://bblais.github.io/posts/2021/Aug/11/stats-101-examples-with-mcmc-part-4/" rel="alternate"></link><published>2021-08-11T00:00:00-04:00</published><updated>2021-08-11T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-08-11:/posts/2021/Aug/11/stats-101-examples-with-mcmc-part-4/</id><summary type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;Estimating mean with known 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="nofollow"&gt;Estimating mean with unknown 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/" rel="nofollow"&gt;Estimating a proportion …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;Estimating mean with known 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="nofollow"&gt;Estimating mean with unknown 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/" rel="nofollow"&gt;Estimating a proportion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all of these posts I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378" rel="nofollow"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/" rel="nofollow"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade
pip install emcee
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, like &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;another  one in the series&lt;/a&gt;, there is some true value, we call &lt;span class="math"&gt;\(x_o\)&lt;/span&gt;.  The &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points we have, &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, consist of that true value drawn from a particular distribution, in this case the &lt;a href="https://en.wikipedia.org/wiki/Cauchy_distribution" rel="nofollow"&gt;Cauchy distribution&lt;/a&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(x|\mu,\gamma)= \frac{1}{\pi\gamma}\left(\frac{\gamma^2}{(x-x_o)^2+\gamma^2}\right)
$$&lt;/div&gt;
&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Pasted image 20210731113326.png"&gt;&lt;/p&gt;
&lt;p&gt;This distribution has &lt;a href="https://en.wikipedia.org/wiki/Cauchy_distribution#Occurrence_and_applications" rel="nofollow"&gt;a number of applications&lt;/a&gt;, many of which occur in cases of circular geometry.  I was introduced to it with the so-called &lt;a href="https://bayes.wustl.edu/sfg/why.pdf" rel="nofollow"&gt;lighthouse problem&lt;/a&gt; and E T Jaynes's excellent &lt;a href="http://bayes.wustl.edu/etj/articles/confidence.pdf" rel="nofollow"&gt;Confidence Intervals vs Bayesian Intervals&lt;/a&gt; paper.&lt;/p&gt;
&lt;p&gt;Although this distribution has a single-peak with a central value, &lt;span class="math"&gt;\(x_o\)&lt;/span&gt;,  and looks vaguely Gaussian, it has many mathematical properties which make it much different.  For example, it has an &lt;em&gt;undefined mean and variance,&lt;/em&gt; and thus doesn't satisfy the conditions for the central limit theorem.   As such it is never covered in introductory statistics classes.  However, with the computational approach described here, it is no harder to work with the Cauchy than the Normal.&lt;/p&gt;
&lt;h2 id="setting-up-the-problem"&gt;Setting up the problem&lt;/h2&gt;
&lt;p&gt;In E T Jaynes's example, the data consist simply of two data points:  &lt;span class="math"&gt;\(\{x_i\} = \{3,5\}\)&lt;/span&gt;.  Remarkably, frequentist methods can't address this problem.  The likelihood is set in the same way as &lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="nofollow"&gt;previous posts&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #6EBF26; font-weight: bold"&gt;def&lt;/span&gt;&lt;span style="color: #666"&gt; &lt;/span&gt;&lt;span style="color: #71ADFF"&gt;lnlike&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data,x_o,γ):&lt;/span&gt;
    &lt;span style="color: #D0D0D0"&gt;x=data&lt;/span&gt;
    &lt;span style="color: #6EBF26; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;logcauchypdf(x,x_o,γ)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;data=array([&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3.&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;5&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;])&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model=MCMCModel(data,lnlike,&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;x_o=Uniform(-&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;),&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;γ=Jeffreys()&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="running-mcmc-looking-at-the-results"&gt;Running MCMC, looking at the results&lt;/h2&gt;
&lt;p&gt;Now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.run_mcmc(&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1500&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,repeat=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model.plot_chains()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sampling Prior...
Done.
0.43 s
Running MCMC 1/3...
Done.
3.74 s
Running MCMC 2/3...
Done.
3.61 s
Running MCMC 3/3...
Done.
3.66 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Pasted image 20210731115421.png"&gt;&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.plot_distributions()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/Pasted image 20210731115437.png"&gt;&lt;/p&gt;
&lt;h2 id="comparing-to-intution-and-conclusions"&gt;Comparing to intution and conclusions&lt;/h2&gt;
&lt;p&gt;Although the Cauchy has no defined mean value, one can show that the &lt;em&gt;median&lt;/em&gt; of the data is a best estimate for the central value, &lt;span class="math"&gt;\(x_o\)&lt;/span&gt; -- although that doesn't give the uncertainties.  Thus, the estimate we find above, &lt;span class="math"&gt;\(x_o \sim 4\)&lt;/span&gt;, matches what we intuit to be the correct answer.&lt;/p&gt;
&lt;p&gt;The point here, however, is that the same Bayesian methods for finding the best estimates and the uncertainties works for this case like all the others -- with very little extra work -- whereas traditional methods fail.  It is telling that statistics textbooks &lt;em&gt;uniformly ignore this entire distribution&lt;/em&gt; as far as I can tell just to avoid the problems with it.  The Bayesian approach, in contrast, provides a uniform procedure to address all such problems and that procedure consistently produces useful results.  &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category><category term="MCMC"></category><category term="Bayesian"></category></entry><entry><title>Stats 101 Examples with MCMC Part 3</title><link href="https://bblais.github.io/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/" rel="alternate"></link><published>2021-05-13T00:00:00-04:00</published><updated>2021-05-13T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-05-13:/posts/2021/May/13/stats-101-examples-with-mcmc-part-3/</id><summary type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;Estimating mean with known 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="nofollow"&gt;Estimating mean with unknown 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  Others in the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;Estimating mean with known 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="nofollow"&gt;Estimating mean with unknown 𝜎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all of these posts I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378" rel="nofollow"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/" rel="nofollow"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade
pip install emcee
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example the data take the form of a number of "successes", &lt;span class="math"&gt;\(h\)&lt;/span&gt;, in a total number of "trials", &lt;span class="math"&gt;\(N\)&lt;/span&gt;, and we want to estimate the proportion, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.  This could be the number of heads observed in &lt;span class="math"&gt;\(N\)&lt;/span&gt; coin flips and we want to estimate the bias in the coin -- what is the probability that it flips heads.  In fact, the data I will use below is from &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents" rel="nofollow"&gt;Lindley, 1976. "Inference for a Bernoulli Process (A Bayesian View)"&lt;/a&gt;, where he flips a thumbtack which might be able to produce two states pointing "up" or pointing "down", and gets the data UUUDUDUUUUUD (3 D, 9 U).&lt;/p&gt;
&lt;h2 id="setting-up-the-problem"&gt;Setting up the problem&lt;/h2&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined from the Bernoulli function,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(h|N,\theta) \sim \text{Bernoulli}(N,\theta)
$$&lt;/div&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the location parameters.  This would translate to 
&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\theta) \sim \text{Uniform}(0,1)
$$&lt;/div&gt;
&lt;p&gt;However there are cases where we have more information.  Even flipping a biased coin, we know that both outcomes (heads and tails) are at least &lt;em&gt;possible&lt;/em&gt; to happen.  Given this, the probabilities &lt;span class="math"&gt;\(P(\theta=0)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(\theta=1)\)&lt;/span&gt; should be zero and a different prior is justified (sidenote -- the maximum entropy solution to this is a Beta(2,2) distribution or, in other words, assume one heads and one tails have been observed before the data).  I'll continue with this example with uniform priors, because in &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents" rel="nofollow"&gt;Lindley, 1976. "Inference for a Bernoulli Process (A Bayesian View)"&lt;/a&gt; he is flipping a particular thumbtack for which it might not be possible for it to, say, flip "down".  His data turns out to be 3 down, 9 up, so we know after the data it was possible for both outcomes but not &lt;em&gt;before&lt;/em&gt; the data.&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #6EBF26; font-weight: bold"&gt;def&lt;/span&gt;&lt;span style="color: #666"&gt; &lt;/span&gt;&lt;span style="color: #71ADFF"&gt;lnlike&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data,θ):&lt;/span&gt;
    &lt;span style="color: #D0D0D0"&gt;h,N=data&lt;/span&gt;
    &lt;span style="color: #6EBF26; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;logbernoullipdf(θ,h,N)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;data=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;12&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model=MCMCModel(data,lnlike,&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;θ=Uniform(&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;0&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;),&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="running-mcmc-looking-at-the-results"&gt;Running MCMC, looking at the results&lt;/h2&gt;
&lt;p&gt;Now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.run_mcmc(&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;800&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,repeat=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model.plot_chains()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sampling Prior...
Done.
0.27 s
Running MCMC 1/3...
Done.
2.63 s
Running MCMC 2/3...
Done.
2.79 s
Running MCMC 3/3...
Done.
2.47 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/proportion chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.plot_distributions()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/proportion distribution.png"&gt;&lt;/p&gt;
&lt;p&gt;Is this evidence for a biased coin at, say, 95% level?&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.P(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;quot;θ &amp;lt;0.5&amp;quot;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #51B2FD"&gt;0.95485&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;yes, just as &lt;a href="https://www.jstor.org/stable/2683855?seq=1#metadata_info_tab_contents" rel="nofollow"&gt;Lindley, 1976 presents.&lt;/a&gt; &lt;/p&gt;
&lt;h2 id="comparing-to-textbook-examples"&gt;Comparing to textbook examples&lt;/h2&gt;
&lt;p&gt;The textbook only considers large sample proportions, with the definition &lt;span class="math"&gt;\(f= h/N\)&lt;/span&gt;. From the Beta distribution we get our estimate of the proportion &lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray}
\hat{\theta} &amp;amp;\sim&amp;amp; f \\
\sigma^2 &amp;amp;\sim&amp;amp; \frac{f\cdot (1-f)}{N}
\end{eqnarray}&lt;/div&gt;
&lt;p&gt;
which matches the equations in the standard textbooks.  Comparing to our case, with &lt;span class="math"&gt;\(N=120\)&lt;/span&gt; we get good agreement,&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare proportion N=120.png"&gt;&lt;/p&gt;
&lt;p&gt;However, we start seeing differences where the textbook treatment isn't as informative as the methods we're using here.  For small samples, say &lt;span class="math"&gt;\(N=12\)&lt;/span&gt;, we see that the textbook approximation breaks down -- especially near the &lt;span class="math"&gt;\(f=0\)&lt;/span&gt; and &lt;span class="math"&gt;\(f=1\)&lt;/span&gt; boundaries, sometimes overestimating our certainty and sometimes underestimating it.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare proportion N=12.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice, however, that in the current approach we didn't need to change anything to get more robust answers.  Also notice that the current process for this problem is identical to the process for all of the other cases we've considered.  So what we are presenting, compared to the textbook treatments, is a more &lt;em&gt;unified&lt;/em&gt; approach which &lt;em&gt;generalizes&lt;/em&gt; to small samples giving a more &lt;em&gt;correct&lt;/em&gt; analysis for our data.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Stats 101 Examples with MCMC Part 2</title><link href="https://bblais.github.io/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/" rel="alternate"></link><published>2021-04-28T00:00:00-04:00</published><updated>2021-04-28T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-04-28:/posts/2021/Apr/28/stats-101-examples-with-mcmc-part-2/</id><summary type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  The previous in the series &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;can be found here&lt;/a&gt;.  In all of these posts I'm going to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is another in the series of "&lt;em&gt;Statistics 101&lt;/em&gt;" examples solved with MCMC.  The previous in the series &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;can be found here&lt;/a&gt;.  In all of these posts I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378" rel="nofollow"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/" rel="nofollow"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &amp;quot;git+git://github.com/bblais/sci378&amp;quot; --upgrade
pip install emcee
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, like &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;the last one&lt;/a&gt;, there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points we have, &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, consist of that true value with added independent, normally-distributed noise of &lt;em&gt;unknown&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined exactly as before,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
x_i &amp;amp;\sim \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim \text{Normal}(x_i-\mu,\sigma) \\
P(\{x_i\}|\mu,\sigma) &amp;amp;\sim \prod_i \text{Normal}(x_i-\mu,\sigma) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
but with &lt;em&gt;unknown&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the location parameters, but &lt;a href="https://en.wikipedia.org/wiki/Jeffreys_prior" rel="nofollow"&gt;Jeffreys&lt;/a&gt; priors on &lt;em&gt;scale&lt;/em&gt; parameters, so we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and Jeffreys priors on &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="setting-up-the-problem"&gt;Setting up the problem&lt;/h2&gt;
&lt;p&gt;In this example, like &lt;a href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="nofollow"&gt;the last one&lt;/a&gt;, there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, with normally-distributed noise but this time with unknown scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  The likelihood function is determined identically as before, &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
x_i &amp;amp;\sim \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim \text{Normal}(\mu,\sigma) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The prior for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, which we must now estimate, is taken to be a &lt;a href="https://en.wikipedia.org/wiki/Jeffreys_prior" rel="nofollow"&gt;Jeffreys prior&lt;/a&gt;.  This prior is uniform in &lt;span class="math"&gt;\(\log \sigma\)&lt;/span&gt; or, in other words, uniform in &lt;em&gt;scale&lt;/em&gt;. &lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #6EBF26; font-weight: bold"&gt;def&lt;/span&gt;&lt;span style="color: #666"&gt; &lt;/span&gt;&lt;span style="color: #71ADFF"&gt;lnlike&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data,μ):&lt;/span&gt;
    &lt;span style="color: #ABABAB; font-style: italic"&gt;# known σ&lt;/span&gt;
    &lt;span style="color: #D0D0D0"&gt;x=data&lt;/span&gt;
    &lt;span style="color: #6EBF26; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;lognormalpdf(x,μ,σ)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;data=array([&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;12.0&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;14&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;16&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;])&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model=MCMCModel(data,lnlike,&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;μ=Uniform(-&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;),&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;σ=Jeffreys(),&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="running-mcmc-looking-at-the-results"&gt;Running MCMC, looking at the results&lt;/h2&gt;
&lt;p&gt;Now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.run_mcmc(&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;800&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,repeat=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model.plot_chains()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sampling Prior...
Done.
0.27 s
Running MCMC 1/3...
Done.
2.63 s
Running MCMC 2/3...
Done.
2.79 s
Running MCMC 3/3...
Done.
2.47 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/μσ chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.plot_distributions()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/μt distribution.png"&gt;
&lt;img alt="" src="https://bblais.github.io/images/σ distribution.png"&gt;&lt;/p&gt;
&lt;h2 id="comparing-to-textbook-examples"&gt;Comparing to textbook examples&lt;/h2&gt;
&lt;p&gt;We compare to textbook solution for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (i.e. Student t-distribution),&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;x,y=model.get_distribution(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;μ&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;plot(x,y,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;μ̂=mean(data)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;N=&lt;/span&gt;&lt;span style="color: #2FBCCD"&gt;len&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;σμ=std(data,ddof=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)/sqrt(N)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;y_pred=[exp(logtpdf(_,N-&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,μ̂,σμ))&lt;/span&gt; &lt;span style="color: #6EBF26; font-weight: bold"&gt;for&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;_&lt;/span&gt; &lt;span style="color: #6EBF26; font-weight: bold"&gt;in&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;x]&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;plot(x,y_pred,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare μ t.png"&gt;&lt;/p&gt;
&lt;p&gt;and the textbook solution for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; (i.e. Chi-square),&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;x,y=model.get_distribution(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;σ&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,bins=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;800&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;plot(x,y,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;V=((data-data.mean())**&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;2&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;).sum()&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;logp=-N*log(x)-V/&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;2&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;/x**&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;2&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;y_pred=exp(logp)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;dx=x[&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;]-x[&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;0&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;]&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;y_pred=y_pred/y_pred.sum()/dx&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;plot(x,y_pred,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;xlim([&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;0&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;20&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare σ.png"&gt;&lt;/p&gt;
&lt;h2 id="tail-area-probabilities"&gt;Tail-area Probabilities&lt;/h2&gt;
&lt;p&gt;We can easily find the tail-area probability, the Bayesian equivalent to Student-t test, for &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.P(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;quot;μ&amp;gt;15&amp;quot;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #51B2FD"&gt;0.037066666666666664&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and for &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, the Bayesian equivalent to Chi-squared test, &lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.P(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;quot;σ&amp;lt;1&amp;quot;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #51B2FD"&gt;0.01985&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Stats 101 Examples with MCMC</title><link href="https://bblais.github.io/posts/2021/Apr/15/stats-101-examples-with-mcmc/" rel="alternate"></link><published>2021-04-15T00:00:00-04:00</published><updated>2021-04-15T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2021-04-15:/posts/2021/Apr/15/stats-101-examples-with-mcmc/</id><summary type="html">&lt;p&gt;I'd like to walk through some of the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples (e.g. estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; with known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, estimating a proportion, etc...) for which we have simple analytical solutions, but …&lt;/p&gt;&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;I'd like to walk through some of the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples (e.g. estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; with known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, estimating a proportion, etc...) for which we have simple analytical solutions, but explore them with &lt;a href="https://www.youtube.com/watch?v=vTUwEu53uzs" rel="nofollow"&gt;MCMC&lt;/a&gt;.  The reason I want to do this is to introduce MCMC on simple cases, which are not much different than more complicated cases, but are better understood.  The second reason is to explore some cases which are nearly as easy, but are never covered in introductory textbooks.  In all of these I'm going to use a python library I make for my science-programming class, &lt;a href="https://github.com/bblais/sci378" rel="nofollow"&gt;stored on github&lt;/a&gt;, and the &lt;a href="https://emcee.readthedocs.io/en/stable/" rel="nofollow"&gt;emcee&lt;/a&gt; library.  Install like:&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &amp;quot;git+https://github.com/bblais/sci378&amp;quot; --upgrade
pip install emcee
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We'll start with the easiest example:  there is some true value, we call &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  The &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points we have, &lt;span class="math"&gt;\(\{x_i\}\)&lt;/span&gt;, consist of that true value with added independent, normally-distributed noise of &lt;em&gt;known&lt;/em&gt; scale, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.  &lt;/p&gt;
&lt;p&gt;For a Bayesian solution, we need to specify the likelihood function -- how our model produces the data -- and the prior probability for the parameters.  The likelihood function is determined like&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
x_i &amp;amp;\sim \mu + \epsilon_i \\
P(x_i|\mu,\sigma) &amp;amp;\sim \text{Normal}(x_i-\mu,\sigma) \\
P(\{x_i\}|\mu,\sigma) &amp;amp;\sim \prod_i \text{Normal}(x_i-\mu,\sigma) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;
known &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the "&lt;em&gt;Statistics 101&lt;/em&gt;" examples, the results are typically equivalent to &lt;em&gt;uniform&lt;/em&gt; priors on the parameters, so we'll assume uniform priors on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.  &lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #6EBF26; font-weight: bold"&gt;def&lt;/span&gt;&lt;span style="color: #666"&gt; &lt;/span&gt;&lt;span style="color: #71ADFF"&gt;lnlike&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data,μ):&lt;/span&gt;
    &lt;span style="color: #ABABAB; font-style: italic"&gt;# known σ&lt;/span&gt;
    &lt;span style="color: #D0D0D0"&gt;x=data&lt;/span&gt;
    &lt;span style="color: #6EBF26; font-weight: bold"&gt;return&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;lognormalpdf(x,μ,σ)&lt;/span&gt;

&lt;span style="color: #D0D0D0"&gt;data=array([&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;12.0&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;14&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;16&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;])&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;σ=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;1&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model=MCMCModel(data,lnlike,&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;μ=Uniform(-&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;50&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;),&lt;/span&gt;
                &lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;now we run MCMC, plot the chains (so we can see it has converged) and look at distributions,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.run_mcmc(&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;800&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;,repeat=&lt;/span&gt;&lt;span style="color: #51B2FD"&gt;3&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;model.plot_chains()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sampling Prior...
Done.
0.21 s
Running MCMC 1/3...
Done.
2.23 s
Running MCMC 2/3...
Done.
2.24 s
Running MCMC 3/3...
Done.
2.51 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/μ chains.png"&gt;&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.plot_distributions()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/μ distribution.png"&gt;&lt;/p&gt;
&lt;p&gt;compare to textbook solution (i.e. Normal with deviation &lt;span class="math"&gt;\(\sigma/\sqrt{N}\)&lt;/span&gt;),&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;x,y=model.get_distribution(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;μ&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;plot(x,y,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;μ̂=mean(data)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;N=&lt;/span&gt;&lt;span style="color: #2FBCCD"&gt;len&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;(data)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;σμ=σ/sqrt(N)&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;y_pred=[exp(lognormalpdf(_,μ̂,σμ))&lt;/span&gt; &lt;span style="color: #6EBF26; font-weight: bold"&gt;for&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;_&lt;/span&gt; &lt;span style="color: #6EBF26; font-weight: bold"&gt;in&lt;/span&gt; &lt;span style="color: #D0D0D0"&gt;x]&lt;/span&gt;
&lt;span style="color: #D0D0D0"&gt;plot(x,y_pred,&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;r--&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="https://bblais.github.io/images/compare μ.png"&gt;&lt;/p&gt;
&lt;p&gt;and finally find a tail-area probability, the Bayesian equivalent to Z-test,&lt;/p&gt;
&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #D0D0D0"&gt;model.P(&lt;/span&gt;&lt;span style="color: #ED9D13"&gt;&amp;#39;μ&amp;gt;15&amp;#39;&lt;/span&gt;&lt;span style="color: #D0D0D0"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight" style="background: #202020"&gt;&lt;pre style="line-height: 125%;"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span style="color: #51B2FD"&gt;0.037066666666666664&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This simple example can be a template for more complex problems, which I will explore in later posts of this series.  What do you think?&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Probability - It's Not Just about the Math</title><link href="https://bblais.github.io/posts/2019/Jun/16/probability-its-not-just-about-the-math/" rel="alternate"></link><published>2019-06-16T00:00:00-04:00</published><updated>2019-06-16T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-06-16:/posts/2019/Jun/16/probability-its-not-just-about-the-math/</id><summary type="html">&lt;p&gt;When I first learned probability, I thought it was all about math and counting.  Then &lt;a href="https://bayes.wustl.edu/etj/etj.html" rel="nofollow"&gt;E. T. Jaynes&lt;/a&gt; showed me that probability forms the foundation of rationality itself.  Remarkably very …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I first learned probability, I thought it was all about math and counting.  Then &lt;a href="https://bayes.wustl.edu/etj/etj.html" rel="nofollow"&gt;E. T. Jaynes&lt;/a&gt; showed me that probability forms the foundation of rationality itself.  Remarkably very few axioms are needed to constrain the mathematical forms necessary for rational thought and the outcome turns out to be &lt;a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#Analytic_theory_of_probabilities" rel="nofollow"&gt;Laplace's original formulation&lt;/a&gt;.  What follows are the axioms so that you can appreciate them as much as I.  The paper &lt;em&gt;&lt;a href="https://bayes.wustl.edu/gregory/articles.pdf" rel="nofollow"&gt;From Laplace to Supernova SN1987A: Bayesian Inference In Astrophysics&lt;/a&gt;&lt;/em&gt; by Tom Loredo is an excellent and complete guide to this, including quantitative examples.  &lt;/p&gt;
&lt;p&gt;Jaynes prefers the word &lt;em&gt;desiderata&lt;/em&gt; for this list - a collection of things needed or wanted - but they function the same as &lt;em&gt;axioms&lt;/em&gt; of the analysis - unproven foundational statements as a basis for a derived system - and I prefer to use that terminology.  Whatever you'd like to name them, here they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Degrees of plausibility are represented by real numbers&lt;/strong&gt;.  This is a choice of practicality.  Perhaps there is some value in probability represented with &lt;em&gt;complex numbers&lt;/em&gt;, but so far no useful generalization has been found to my knowledge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qualitative consistency with common sense&lt;/strong&gt;.  Specifically, the system must be consistent with standard Boolean logic - the logic of syllogisms and deductive logic.  This is not a generic call to "&lt;em&gt;common sense&lt;/em&gt;" to base the system on, but rather the short-hand informal speech that Jaynes likes to use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Internal consistency&lt;/strong&gt;. If a conclusion can be reasoned out in more than one way then every possible way must yield the same result.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Propriety&lt;/strong&gt;.  We must take into account all of the information provided that is relevant to the question.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent states consistency&lt;/strong&gt;.  Equivalent states of knowledge must be represented by equivalent plausibility assignments.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To quote Jaynes: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At this point, most students are surprised to learn that our search for desiderata is at an end. The above conditions, it turns out, uniquely determine the rules by which [we] must reason; i.e. there is only one set of mathematical operations for manipulating plausibilities which has all these properties.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From these axioms, we can derive,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the sum and product rules of probability, which describe the process of combining bits of knowledge&lt;/li&gt;
&lt;li&gt;Bayes Rule, which describes the structure of learning from evidence&lt;/li&gt;
&lt;li&gt;the marginalization rule, where model simplicity is attributed to fewer adjustable parameters&lt;/li&gt;
&lt;li&gt;deductive logic, the limit as the probabilities go to 0 and 1&lt;/li&gt;
&lt;li&gt;the dangers of either/or thinking&lt;/li&gt;
&lt;li&gt;and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An important consequence of the derivation is that &lt;em&gt;any&lt;/em&gt; system which disagrees with it must violate one or more of the axioms listed.  The entire approach shows that probability theory is far more fundamental than is typically appreciated.&lt;/p&gt;</content><category term="math"></category></entry><entry><title>An Interesting Twist on the Gambler's Fallacy</title><link href="https://bblais.github.io/posts/2019/Mar/24/an-interesting-twist-on-the-gamblers-fallacy/" rel="alternate"></link><published>2019-03-24T00:00:00-04:00</published><updated>2019-03-24T00:00:00-04:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-03-24:/posts/2019/Mar/24/an-interesting-twist-on-the-gamblers-fallacy/</id><summary type="html">&lt;p&gt;I was reminded of the &lt;a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy" rel="nofollow"&gt;Gambler's Fallacy&lt;/a&gt; in a &lt;a href="https://theness.com/neurologicablog/index.php/the-gamblers-fallacy/" rel="nofollow"&gt;recent post by Stephen Novella&lt;/a&gt; and thought of a twist on the fallacy that makes it a bit more subtle.  The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was reminded of the &lt;a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy" rel="nofollow"&gt;Gambler's Fallacy&lt;/a&gt; in a &lt;a href="https://theness.com/neurologicablog/index.php/the-gamblers-fallacy/" rel="nofollow"&gt;recent post by Stephen Novella&lt;/a&gt; and thought of a twist on the fallacy that makes it a bit more subtle.  The fallacy is defined,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the mistaken belief that, if something happens more frequently than normal during a given period, it will happen less frequently in the future (or vice versa). In situations where the outcome being observed is truly random and consists of independent trials of a random process, this belief is false.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you observe 5 heads in a row, for example, then the probability of another heads is still &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt;. In other words it is not more (or less) likely to get a tails after a long string of heads in a row. There are however a couple of cases where this doesn't hold.  &lt;/p&gt;
&lt;h2 id="100-heads"&gt;100 heads&lt;/h2&gt;
&lt;p&gt;What happens if you observe someone flipping a coin and they achieve 100 heads in a row.  Would you really think that  the probability of another heads is still &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt;?  Is it even &lt;em&gt;rational&lt;/em&gt; to believe that?  At some point, you would question the &lt;em&gt;independence of trials&lt;/em&gt; assumption.  You might think that the coin is rigged (two heads?) or that the flipping process is rigged (spinning the coin like a frisbee instead of end-over-end motion) or something else.  The mathematics for it is pretty straightforward.  Assume you have two models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_1\)&lt;/span&gt;: the coin-flipping process is random, trials are independent, and the probability of each flip is &lt;span class="math"&gt;\(p=0.5\)&lt;/span&gt; for heads&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M_2\)&lt;/span&gt;: the coin has two heads, so no matter what the flipping process is the probability of getting heads is &lt;span class="math"&gt;\(p=1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and our data is &lt;span class="math"&gt;\(m\)&lt;/span&gt; heads flips in a row&lt;/p&gt;
&lt;div class="math"&gt;$$
D \equiv \left\{\underbrace{H_1,H_2,H_3,\cdots,H_m}_{\mbox{$m$ flips}}\right\}
$$&lt;/div&gt;
&lt;p&gt;We are then interested in whether the &lt;span class="math"&gt;\(m+1\)&lt;/span&gt; flip is a heads, or &lt;span class="math"&gt;\(P(H_{m+1}|D)\)&lt;/span&gt;.  Since we have two possible models which could give this outcome, we break this probability into a sum for each,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(H_{m+1}|D) = P(H_{m+1}|D,M_1)P(M_1|D) + P(H_{m+1}|D,M_2)P(M_2|D)
$$&lt;/div&gt;
&lt;p&gt;Since in both models the history of flips makes no difference to the next flip we get&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
P(H_{m+1}|D,M_1) &amp;amp;= P(H_{m+1}|M_1) = 0.5\\
P(H_{m+1}|D,M_2) &amp;amp;= P(H_{m+1}|M_2) = 1
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;So, the bulk of the probability comes from the probability of each model given the data.  Applying Bayes' Rule, we get &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
P(M_1|D) &amp;amp;\sim P(D|M_1)P(M_1)=\frac{0.5^m \cdot P(M_1)}{T} \\
P(M_2|D) &amp;amp;\sim P(D|M_2)P(M_2)=\frac{1^m \cdot P(M_2)}{T}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(T\)&lt;/span&gt; is the total probability of the data and the probabilities &lt;span class="math"&gt;\(P(M_1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(M_2)\)&lt;/span&gt; are the priors for Model 1 and Model 2, respectively.  &lt;/p&gt;
&lt;p&gt;We can start figuring out the prior probability by thinking in the following way.  Since we usually don't think a system is rigged unless we start seeing a strong pattern consistent with that, we expect that the prior for &lt;span class="math"&gt;\(M_2\)&lt;/span&gt; should be &lt;em&gt;much less&lt;/em&gt; than for &lt;span class="math"&gt;\(M_1\)&lt;/span&gt;.  Without loss of generality, we assign &lt;span class="math"&gt;\(P(M_1) = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(M_2) = 10^{-a}\)&lt;/span&gt;.  If &lt;span class="math"&gt;\(a=3\)&lt;/span&gt; then Model 2 is 1000x less likely (before the data) than Model 1,  if &lt;span class="math"&gt;\(a=6\)&lt;/span&gt; then Model 2 is 1 million times less likely, etc...  We  consider that the coin might not be independent when the probabilities for the two models -- after the data -- become comparable.  Each heads observation makes Model 1 a bit less likely and Model 2 a bit more so.  In a picture (for the case of &lt;span class="math"&gt;\(a=6\)&lt;/span&gt;) we have&lt;/p&gt;
&lt;p&gt;&lt;img alt="coin_flips_rigged.png" class="img-fluid" src="https://bblais.github.io/images/coin_flips_rigged.png"&gt;&lt;/p&gt;
&lt;p&gt;Mathematically we have&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(M_2|D)}{P(M_1|D)} = \frac{10^{-a}}{0.5^m} \gt 1
$$&lt;/div&gt;
&lt;p&gt;which yields the number of flips, &lt;span class="math"&gt;\(m\)&lt;/span&gt;, to overcome a prior against the two-headed coin, &lt;span class="math"&gt;\(a\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
m&amp;gt; \frac{a}{\log_{10} 2}
$$&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(a=6\)&lt;/span&gt; (or initially a million times less likely) then &lt;span class="math"&gt;\(m&amp;gt;19.9\)&lt;/span&gt; flips is needed to overcome that level of initial skepticism. &lt;/p&gt;
&lt;h2 id="alternating-results-or-quasi-random-processes"&gt;Alternating Results or Quasi-random processes&lt;/h2&gt;
&lt;p&gt;The same sort of analysis can occur if instead of a string of heads we observe a perfectly oscillating sequence (H,T,H,T,H,T,...).  This, of course, could be the result of a random process but if it continued long enough then even a low prior of a rigged system could be overcome.  &lt;/p&gt;
&lt;p&gt;The same could occur if you happen to be observing a &lt;a href="https://en.wikipedia.org/wiki/Sobol_sequence" rel="nofollow"&gt;Sobol Sequence&lt;/a&gt; or some other &lt;a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence" rel="nofollow"&gt;quasi-random processes&lt;/a&gt;. These sequences are &lt;em&gt;designed&lt;/em&gt; to be non-independent in order to be more likely to fill the space of numbers evenly.  Thus, if there is a sequence of 5 heads in a row, then under a quasi-random process a tails is in fact &lt;em&gt;more likely&lt;/em&gt; in these sequences as the "fallacy" warns us against.  These processes have some use in sampling for simulations where you want to make sure to cover all the the values of the parameter space evenly and efficiently.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I am not arguing that the Gambler's Fallacy is wrong, nor am I arguing that we should think every case is rigged.  I am further not suggesting that people are actually good at these problems - they aren't.  We see patterns in random sequences and events all of the time, and perhaps we should be a bit more reticent to ascribe non-randomness to processes for which we have no evidence of a non-random influence.  In other words, our priors for a rigged system should generally be quite low.  However, there can be cases where the system &lt;em&gt;is rigged&lt;/em&gt; and we should be open to that possibility as well.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>If at first you don't succeed (for 50 times) you might reconsider</title><link href="https://bblais.github.io/posts/2019/Jan/29/if-at-first-you-dont-succeed-for-50-times-you-might-reconsider/" rel="alternate"></link><published>2019-01-29T00:00:00-05:00</published><updated>2019-01-29T00:00:00-05:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2019-01-29:/posts/2019/Jan/29/if-at-first-you-dont-succeed-for-50-times-you-might-reconsider/</id><summary type="html">&lt;p&gt;Here's a &lt;a href="https://www.quora.com/If-there-is-a-25-chance-of-something-what-is-the-probability-that-you-fail-to-succeed-50-times-in-a-row-before-you-finally-do-succeed/answer/Brian-Blais" rel="nofollow"&gt;straightforward problem&lt;/a&gt; from Quora - probably found in a textbook - but I try to find a new direction to make it more interesting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If there is a 25% chance …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;Here's a &lt;a href="https://www.quora.com/If-there-is-a-25-chance-of-something-what-is-the-probability-that-you-fail-to-succeed-50-times-in-a-row-before-you-finally-do-succeed/answer/Brian-Blais" rel="nofollow"&gt;straightforward problem&lt;/a&gt; from Quora - probably found in a textbook - but I try to find a new direction to make it more interesting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If there is a 25% chance of something, what is the probability that you fail to succeed 50 times in a row before you finally do succeed?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem, as stated, gives you a very small probability of 50 failures followed by a success.  As stated, this probability is simply&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(S_{51}, F_{1,2,\cdots,50}) = 0.75^{50} \times 0.25 \sim 10^{-7}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, we rarely know that an event has &lt;em&gt;exactly&lt;/em&gt; a probability of 25% - this is usually just a hunch.  Or possibly we &lt;em&gt;believe&lt;/em&gt; the probability to be 25% but in fact it’s actually 0% or in other words (unknowingly) impossible - the claim itself can be wrong.  Further, the way the question is worded almost implies that you're trying to decide to abandon...or not...an action after 50 failures.  This is a different question than the original, I admit, but much closer to what one might expect in a more realistic scenario.&lt;/p&gt;
&lt;p&gt;Pursuing this adjusted question, say we have the following two possibilities: the original claim that &lt;span class="math"&gt;\(p_{\rm event}=0.25\)&lt;/span&gt; or the claim is false and (for example) &lt;span class="math"&gt;\(p_{\rm event}=0\)&lt;/span&gt;.  Let’s further assume that we are quite sure of the original claim, at level of 99.99%.  With the new question, we are interested in the probability of a success on try 51 after 50 previous failures, which is now a combination of the result of the claim possibly being true and the claim possibly being false.  I will outline the process for &lt;span class="math"&gt;\(m\)&lt;/span&gt; failures instead of 50, so that we can discuss it in a more interesting way.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(S_{51}| F_{1,2,\cdots,50}) = P(S_{51}| F_{1,2,\cdots,m}, {\rm True\ Claim})P({\rm True\ Claim}|F_{1,2,\cdots,m}) + P(S_{51}| F_{1,2,\cdots,m}, {\rm False\ Claim})P({\rm False\ Claim}|F_{1,2,\cdots,m})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key part of this calculation is the comparison of the two possibilities (i.e. the claim is true or the claim is false) after observing 50 failures. We can apply Bayes Rule, &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P({\rm True\ Claim}|F_{1,2,\cdots,m}) \sim P(F_{1,2,\cdots,m}| {\rm True\ Claim})\cdot P({\rm True\ Claim}) \sim 0.75^m \cdot 0.9999\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P({\rm False\ Claim}|F_{1,2,\cdots,m}) \sim P(F_{1,2,\cdots,m}| {\rm False\ Claim})\cdot P({\rm False\ Claim})\sim 1 \cdot 0.0001\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After only 32 failures these two probabilities are equal, or in other words, it's even money that the action is even &lt;em&gt;possible&lt;/em&gt;.  After 50 failures, the odds for the original claim are around 180:1 against.  Clearly we are facing the definition of insanity&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="nofollow"&gt;1&lt;/a&gt;&lt;/sup&gt; at this point.&lt;/p&gt;
&lt;p&gt;Essentially, because 50 failures is so unlikely for a 25% chance event or action, other possibilities intrude - such as "success &lt;em&gt;at all&lt;/em&gt; is impossible".  There can, of course, be other possibilities that come to mind after many failures beyond the one I explore here.  However, the point that I want to stress is that one should think a little more deeply when faced even with such "simple" problems.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;doing the same thing over and over expecting a different result.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text" rel="nofollow"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &amp;lt; 768) ? "left" : align;
        indent = (screen.width &amp;lt; 768) ? "0em" : indent;
        linebreak = (screen.width &amp;lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    &lt;a href="http://mathjaxscript.id" rel="nofollow"&gt;mathjaxscript.id&lt;/a&gt; = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '&lt;a href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';" rel="nofollow"&gt;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';&lt;/a&gt;

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="math"></category></entry><entry><title>Cool theorem - angle bisector</title><link href="https://bblais.github.io/posts/2018/Jan/29/cool-theorem-angle-bisector/" rel="alternate"></link><published>2018-01-29T00:00:00-05:00</published><updated>2018-01-29T00:00:00-05:00</updated><author><name>Brian Blais</name></author><id>tag:bblais.github.io,2018-01-29:/posts/2018/Jan/29/cool-theorem-angle-bisector/</id><summary type="html">&lt;p&gt;I was helping my daughter with geometry the other day and &lt;a href="https://en.wikipedia.org/wiki/Angle_bisector_theorem" rel="nofollow"&gt;this cute theorem popped up in the examples&lt;/a&gt;.  For some reason I don't recall ever seeing it before, and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was helping my daughter with geometry the other day and &lt;a href="https://en.wikipedia.org/wiki/Angle_bisector_theorem" rel="nofollow"&gt;this cute theorem popped up in the examples&lt;/a&gt;.  For some reason I don't recall ever seeing it before, and it struck me as somehow particularly elegant.  I have used (many times) the specific - &lt;a href="http://ceemrr.com/Geometry1/RightTriangles/RightTriangles4.html" rel="nofollow"&gt;more limited version with isosceles triangles&lt;/a&gt; - but to see the generalization was just neat.&lt;/p&gt;
&lt;p&gt;It makes me wonder also why I find enjoyment in something this simple and technical.  &lt;/p&gt;</content><category term="math"></category></entry></feed>